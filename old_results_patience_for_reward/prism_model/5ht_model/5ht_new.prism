dtmc

const int enter_pf; //int defining the boundary of the pf associated with reward
const int wait_at_reward_location; //int defining boundary of area "next to" reward location, where the reward will appear.
const int l; //size of the scale representing the entire explorable area (add 10 as a safety buffer) reward is at position l-10
const int delay; //number of steps to wait to spawn reward once agent waits at reward_location
const int reward_lifetime; //number of steps to wait before reward despawns after its instantiation

const double see_reward_approach_perpendicular = 1;
const double see_reward_approach_pi_over_8 = 0.5;
const double see_reward_approach_mt_pi_over_4 = 0;
const double see_reward_approach_neg_pi_over_8 = 0.5;
const double see_reward_approach_neg_lt_pi_over_4 = 0;

global reward_present : bool init false; 
global in_front_of_reward_location : bool init false; //agent is slowing in front of the reward location 


module limbic_system



vis1 : [0..1] init 0;//visual input to speed gained when agent can see reward_location
vis2: [0..1];//  visual input to speed gained when agent can see reward
_5ht : [0..3] init 0;// serotonin/5HTR1/5HTR2 value
pos : [0..200] init 0;//position of agent

angle_id : [0..6] init 0; //angle identifier: used to specify what agent's angle of approach to the placefield will be.

s : [0..10] init 0;  //state counter to help ordering of events ->
//s=0 -> set angle
//s=1 -> exploration
//s=2 -> has seen visual landmark, approaches
//s=3 -> at placefield, increased HT
//s=4 -> slowed down
//s=5 -> moving towards location of reward
//s=6 -> at end



approach_angle_correct : bool init false;
at_reward : bool init false;
collected_reward : bool init false;
missed_reward : bool init false;

[] s=0 -> 0.2 : (angle_id'=1) & (s'=1) + 0.2 : (angle_id'=2) & (s'=1) + 0.2 : (angle_id'=3) & (s'=1) + 0.2 : (angle_id'=4) & (s'=1) + 0.2 : (angle_id'= 5) & (s'=1);

[] s=1 -> 0.2 : (vis1'=1) & (s'=2) + 0.8 : (vis1'=0) & (s'=1); // see area containing reward and end exploration

[timed] pos<(l-10) & s>=1 & s<5 -> 1: (pos'=pos+speed); //update position by speed each step

// "timed" syncronises agent movement with the counters controlling the spawning and despawning of the reward


// choose an angle of approach using the angle counter and use the corresponding probablilty of seeing the reward for the approach.

[] s=2 & pos>=enter_pf & angle_id=1 -> see_reward_approach_perpendicular : (_5ht'=1) & (s'=3) + (1-see_reward_approach_perpendicular) : (missed_reward'=true) & (s'=6); //enter the placefield and increase 5ht (increase to 3!!!)
[] s=2 & pos>=enter_pf & angle_id=2  -> see_reward_approach_pi_over_8 : (_5ht'=1) & (s'=3) + (1-see_reward_approach_pi_over_8) : (missed_reward'=true) & (s'=6);
[] s=2 & pos>=enter_pf & angle_id=3 -> see_reward_approach_mt_pi_over_4 : (_5ht'=1) & (s'=3) + (1-see_reward_approach_mt_pi_over_4) : (missed_reward'=true) & (s'=6);
[] s=2 & pos>=enter_pf & angle_id=4 -> see_reward_approach_neg_pi_over_8 : (_5ht'=1) & (s'=3) + (1-see_reward_approach_neg_pi_over_8) : (missed_reward'=true) & (s'=6);
[] s=2 & pos>=enter_pf & angle_id=5 -> see_reward_approach_neg_lt_pi_over_4 : (_5ht'=1) & (s'=3) + (1-see_reward_approach_neg_lt_pi_over_4) : (missed_reward'=true) & (s'=6);

[] s=3 & pos>=wait_at_reward_location -> 1 : (_5ht'=3) & (s'=4) & (in_front_of_reward_location'=true); //slow down if front of reward_location, expected to contain reward

[timed] s=4 & reward_present=true & vis2=0 -> 1 : (vis2'=1); //agent sees reward
[timed] s=4 & pos>=(l-10) -> 1: (s'=5); //agent reaches reward location; update state counter


[timed] s=5 & reward_present=true -> (collected_reward' = true) & (s'=6); //agent collects reward
[timed] s=5  & reward_present=false -> (missed_reward'=true) & (s'=6); //agent misses reward

[] s=6 -> 1: (s'=6);


//when s==6, end state has been reached




endmodule 

module reward_spawner


c1 : [0..delay] init 0; //reward spawn delay counter
c2 : [0..reward_lifetime] init 0; //reward lifetime counter
spwnd : bool init false;
despwnd : bool init false;

[timed] in_front_of_reward_location=false -> 1: (c1'=0);
[timed] in_front_of_reward_location=true & spwnd=false & c1=delay -> 1 : (spwnd'=true) & (c1'=0); //spawn reward after "delay" steps
[timed] in_front_of_reward_location=true & spwnd=false & c1<delay-> 1: (c1'=c1+1);  //increment reward spawn counter

[timed] in_front_of_reward_location=true & spwnd=true & c2=reward_lifetime -> 1 : (despwnd'=true); //despawn reward after "reward_lifetime" steps
[timed] in_front_of_reward_location=true & spwnd=true & c2<reward_lifetime -> 1 : (c2'=c2+1); //increment reward despawn counter

[] spwnd=true & despwnd=false -> 1: (reward_present'=true); //spawn reward
[] despwnd=true -> 1: (reward_present'=false); //despawn reward

endmodule





formula x = vis1 + vis2; //speed based on visual inputs

formula speed = ((_5ht=0 & x=1)?5:(_5ht=1 & x=1)?6:(_5ht=1 & x=2)?8:(_5ht=3 & x=1)?1:(_5ht=3 & x=2)?10:0);//speed lookup based on Figure 3 in Bernd paper (values used here are roughly 10 times the value in the graph)
//these speeds can easily be modified to reflect those of the nutt model

